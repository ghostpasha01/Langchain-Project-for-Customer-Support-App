{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd67349b-5ee8-4301-a1cd-01f4c758d4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install numexpr\n",
    "# !pip install langchain\n",
    "# !pip install openai\n",
    "# !pip install openai-whisper\n",
    "# !pip install sentence-transformers\n",
    "# !pip install unstructured\n",
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa32158-b701-484f-b956-e5081c188bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import random\n",
    "import re\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0164df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = \"enter-your-api-key\"\n",
    "\n",
    "# Set the OpenAI API key as an environment variable\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c219fd4-cd9d-4c88-b8c9-dd9477e5b81e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.chains import LLMChain, LLMMathChain, SequentialChain, TransformChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.tools import Tool\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bd8e7-23b0-49a6-8590-3b04c0e4d656",
   "metadata": {},
   "source": [
    "# Langchain 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a8456-d7ab-4fde-9be5-6eb13eb72b6a",
   "metadata": {},
   "source": [
    "We first create a `PromptTemplate` class and initialize a templat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb60c4a9-624e-461b-88ad-24912c19d4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# user question\n",
    "question = \"Which NFL team won the Super Bowl in the 2010 season?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581124ac-afda-4896-8fcd-1fd3dc48407c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Which NFL team won the Super Bowl in the 2010 season?\\n\\nAnswer: '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73ce74b-40c8-4667-8d15-0b4626bfeeca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "temperature = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba09271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc93d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca10b9ab-8603-4998-978c-4545059b6159",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Green Bay Packers won Super Bowl XLV in the 2010 season.\n"
     ]
    }
   ],
   "source": [
    "# ask the user question about NFL 2010\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19155b80-e1f3-4d48-a662-0ec872dcab0a",
   "metadata": {},
   "source": [
    "Asking multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8099189e-248e-4904-a3fc-bb6852e08831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {\"question\": \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {\"question\": \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {\"question\": \"Who was the 12th person on the moon?\"},\n",
    "    {\"question\": \"How many eyes does a blade of grass have?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c789afb6-64ad-4ba2-a4b5-d679f3e7a5ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = llm_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b94685b7-2348-4a29-86e6-f1e1194dfc13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Green Bay Packers won Super Bowl XLV in the 2010 season.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generations[0][0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a0e5f8-40f5-4dae-b0ce-05edaa7946af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You would be 193.04 centimeters tall.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generations[1][0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "141f8441-0817-4083-bbe1-667bf458b387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison Schmitt was the 12th person to walk on the moon during the Apollo 17 mission in December 1972.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generations[2][0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "093be5eb-ab59-43d7-a833-b653e5a5038b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A blade of grass does not have any eyes. Grass does not have sensory organs like eyes.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generations[3][0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae47fe-170c-4582-9f54-b7a28f86cef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What are chains anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c8589-ac5a-46d9-8641-807ddf8d6973",
   "metadata": {},
   "source": [
    "Definition: Chains are one of the fundamental building blocks of this lib (as you can guess!).\n",
    "\n",
    "The official definition of chains is the following:\n",
    "\n",
    ">A chain is made up of links, which can be either primitives or other chains. Primitives can be either prompts, llms, utils, or other chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561c9bb-b610-4289-9af0-7d8398dddfc9",
   "metadata": {},
   "source": [
    "So a chain is basically a pipeline that processes an input by using a specific combination of primitives. Intuitively, it can be thought of as a 'step' that performs a certain set of operations on an input and returns the result. They can be anything from a prompt-based pass through a LLM to applying a Python function to an text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5181a-d810-4cd4-a671-ad2176f9ebb6",
   "metadata": {},
   "source": [
    "Chains are divided in three types: Utility chains, Generic chains and Combine Documents chains. In this edition, we will focus on the first two since the third is too specific (will be covered in due course).\n",
    "\n",
    "1. Utility Chains: chains that are usually used to extract a specific answer from a llm with a very narrow purpose and are ready to be used out of the box.\n",
    "2. Generic Chains: chains that are used as building blocks for other chains but cannot be used out of the box on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451dbca6-a9bf-42a4-a33f-5baa08fcc81f",
   "metadata": {},
   "source": [
    "### Utility Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1a3c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math = LLMMathChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ae3652a-d519-4990-a581-73306f9c6599",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m```text\n",
      "13**0.3432\n",
      "```\n",
      "...numexpr.evaluate(\"13**0.3432\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 2.4116004626599237'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math.run(\"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc9841-3737-4e51-9e2e-703cb657c74c",
   "metadata": {},
   "source": [
    "Let's see what is going on here. The chain recieved a question in natural language and sent it to the llm. The llm returned a Python code which the chain compiled to give us an answer. A few questions arise.. How did the llm know that we wanted it to return Python code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c66853-fab7-42ff-8256-1c0f860418fb",
   "metadata": {},
   "source": [
    "#### Enter Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0c362-cc0b-4df8-abc5-697d55110a0e",
   "metadata": {},
   "source": [
    "The question we send as input to the chain is not the only input that the llm recieves ðŸ˜‰. The input is inserted into a wider context, which gives precise instructions on how to interpret the input we send. This is called a prompt. Let's see what this chain's prompt is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13a065d9-f456-49c2-b842-c243eb7ce2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_math.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee35a8-e405-4b7c-8ba3-e2796de3d51d",
   "metadata": {},
   "source": [
    "Ok.. let's see what we got here. So, we are literally telling the llm that for complex math problems it should not try to do math on its own but rather it should print a Python code that will calculate the math problem instead. Probably, if we just sent the query without any context, the llm would try (and fail) to calculate this on its own. Wait! This is testable.. let's try it out! ðŸ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb95e9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13 raised to the 0.3432 power is approximately 2.732.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we set the prompt to only have the question we ask\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# we ask the llm for the answer with no context\n",
    "\n",
    "llm_chain.run(\"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a826ab96-eed1-4f0b-8cd4-fcd4a3d28782",
   "metadata": {},
   "source": [
    "Wrong answer! Herein lies the power of prompting and one of our most important insights so far:\n",
    "\n",
    "Insight: by using prompts intelligently, we can force the llm to avoid common pitfalls by explicitly and purposefully programming it to behave in a certain way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a51e5-d965-49f9-981b-749dc824d097",
   "metadata": {},
   "source": [
    "### Generic Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e1d5d-5f33-437d-92fd-b6a12cd36597",
   "metadata": {},
   "source": [
    "There are only three Generic Chains in langchain and we will go all in to showcase them all in the same example. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488f96b-529c-4c4a-81da-de25ff52119d",
   "metadata": {},
   "source": [
    "Say we have had experience of getting dirty input texts. Specifically, as we know, llms charge us by the number of tokens we use and we are not happy to pay extra when the input has extra characters. Plus its not neat ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6862dfc-b18f-4a51-9d9a-fb6b33653976",
   "metadata": {},
   "source": [
    "First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "216e06a8-6739-4d56-969b-69a115c9e79d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "\n",
    "    # replace multiple new lines and multiple spaces with a single one\n",
    "    text = re.sub(r\"(\\r\\n|\\r|\\n){2,}\", r\"\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    return {\"output_text\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa864e6-57ff-4746-8745-bef9434cabf3",
   "metadata": {},
   "source": [
    "Importantly, when we initialize the chain we do not send an llm as an argument. As you can imagine, not having an llm makes this chain's abilities much weaker than the example we saw earlier. However, as we will see next, combining this chain with other chains can give us highly desirable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b01b6119-9779-40a9-8eac-a8bce1f9505e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_extra_spaces_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b1ba995-9223-40fd-be09-2aedabc5f6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A random text with some irregular spacing.\\n Another one here as well.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_extra_spaces_chain.run(\n",
    "    \"A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73788f34-50c3-4983-87ae-939790434e38",
   "metadata": {},
   "source": [
    "Great! Now things will get interesting.\n",
    "\n",
    "Say we want to use our chain to clean an input text and then paraphrase the input in a specific style, say a poet or a policeman. As we now know, the TransformChain does not use a llm so the styling will have to be done elsewhere. That's where our LLMChain comes in. We know about this chain already and we know that we can do cool things with smart prompting so let's take a chance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f4b7709-aaee-4d15-99e7-80fb6f05bee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Paraphrase this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "In the style of a {style}.\n",
    "\n",
    "Paraphrase: \"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f91ba-e3fc-4979-96a9-f06e2aefca37",
   "metadata": {},
   "source": [
    "And next, initialize our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa7b9a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_paraphrase_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"final_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec52eb-9dfd-4f45-9464-4193b4327a0d",
   "metadata": {},
   "source": [
    "Great! Notice that the input text in the template is called 'output_text'. Can you guess why?\n",
    "\n",
    "We are going to pass the output of the `TransformChain` to the `LLMChain`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9663730f-f9f9-4ca0-9918-cb419f6edcae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(\n",
    "    chains=[clean_extra_spaces_chain, style_paraphrase_chain],\n",
    "    input_variables=[\"text\", \"style\"],\n",
    "    output_variables=[\"final_output\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149ab54-7a78-4269-b6a5-b5e1df9d62d5",
   "metadata": {},
   "source": [
    "Our input is the langchain docs description of what chains are but dirty with some extra spaces all around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6538b907-a669-461f-8ba4-765523ddd7b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Chains allow us to combine multiple \n",
    "\n",
    "\n",
    "components together to create a single, coherent application. \n",
    "\n",
    "For example, we can create a chain that takes user input,       format it with a PromptTemplate, \n",
    "\n",
    "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by \n",
    "\n",
    "\n",
    "combining chains with other components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe35989-f97c-4e96-811e-c3511f1a3461",
   "metadata": {},
   "source": [
    "We are all set. Time to get creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4baf9120-8dcd-4022-8eed-753619056c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links intertwine to unite\n",
      "Various parts in harmony's light\n",
      "User input transformed with grace\n",
      "Passed along in a seamless embrace\n",
      "Chains combined, complexity grows\n",
      "Creating beauty only a poet knows\n"
     ]
    }
   ],
   "source": [
    "print(sequential_chain.run({\"text\": input_text, \"style\": \"a poet\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef7fad-46e5-4b02-abd0-2de7ffd9952b",
   "metadata": {},
   "source": [
    "## Langchain Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e214440-0f49-4d0a-9b79-db752f0edb7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Define the data structure we want to be parsed out from the LLM response\n",
    "\n",
    "notice that the class contains a setup (a string) and a punchline (a string.\n",
    "The descriptions are used to construct the prompt to the llm. This particular\n",
    "example also has a validator which checks if the setup contains a question mark.\n",
    "\n",
    "from: https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "868183c3-0de5-4c76-8e71-271bda514b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Defining the query from the user\n",
    "\"\"\"\n",
    "joke_query = \"Tell me a joke about parrots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc6c94d8-dec3-4a4d-82a0-83a6179cc268",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the user query.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n",
      "Tell me a joke about parrots\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Defining the prompt to the llm\n",
    "\n",
    "from: https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic\n",
    "\"\"\"\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(query=joke_query)\n",
    "\n",
    "print(_input.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d69c93e-5751-49ab-a6d7-5390732b33dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why do parrots love to talk so much?', punchline='Because they always want to beak the record!')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Declaring a model and querying it with the parser defined input\n",
    "\"\"\"\n",
    "\n",
    "output = llm_chain.run(_input.to_string())\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ac28e-4d50-421c-ab0c-44f43f476a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c4a7dfe-c66c-4b7e-a5ef-d5c84f929155",
   "metadata": {},
   "source": [
    "# LLMs as Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d1313d9-0bcb-4587-bb3d-78b0ce383349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining the model used in this test\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "temperature = 0.0\n",
    "model = ChatOpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6ce8561-27f4-4150-ad7b-a3489aa23b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class sampleOutputTemplate(BaseModel):\n",
    "    output: str = Field(description=\"contact information\")\n",
    "\n",
    "\n",
    "class Validation(BaseModel):\n",
    "    is_valid: bool = Field(description=\"if the condition is satisfied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4598a-ac46-45ab-afe5-86cf5857f67a",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d4e2a85-dbc0-4168-91c6-6e6efa90d525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Defining utility functions for constructing a readable exchange\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def system_output(output):\n",
    "    \"\"\"Function for printing out to the user\"\"\"\n",
    "    print(\"======= Bot =======\")\n",
    "    print(output)\n",
    "\n",
    "\n",
    "def user_input():\n",
    "    \"\"\"Function for getting user input\"\"\"\n",
    "    print(\"======= Human Input =======\")\n",
    "    return input()\n",
    "\n",
    "\n",
    "def parsing_info(output):\n",
    "    \"\"\"Function for printing out key info\"\"\"\n",
    "    print(f\"*Info* {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae788b-b042-4e79-9597-e3cbb72a2bec",
   "metadata": {},
   "source": [
    "## Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b432cda-68af-4f4f-b0db-e2229f5133ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Edge:\n",
    "\n",
    "    \"\"\"Edge\n",
    "    at its highest level, an edge checks if an input is good, then parses\n",
    "    data out of that input if it is good\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, condition, parse_prompt, parse_class, llm, max_retrys=3, out_node=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        condition (str): a True/False question about the input\n",
    "        parse_query (str): what the parser whould be extracting\n",
    "        parse_class (Pydantic BaseModel): the structure of the parse\n",
    "        llm (LangChain LLM): the large language model being used\n",
    "        \"\"\"\n",
    "        self.condition = condition\n",
    "        self.parse_prompt = parse_prompt\n",
    "        self.parse_class = parse_class\n",
    "        self.llm = llm\n",
    "\n",
    "        # how many times the edge has failed, for any reason, for deciding to skip\n",
    "        # when successful this resets to 0 for posterity.\n",
    "        self.num_fails = 0\n",
    "\n",
    "        # how many retrys are acceptable\n",
    "        self.max_retrys = max_retrys\n",
    "\n",
    "        # the node the edge directs towards\n",
    "        self.out_node = out_node\n",
    "\n",
    "    def check(self, input):\n",
    "        \"\"\"ask the llm if the input satisfies the condition\"\"\"\n",
    "        validation_query = f\"following the output schema, does the input satisfy the condition?\\ninput:{input}\\ncondition:{self.condition}\"\n",
    "\n",
    "        class Validation(BaseModel):\n",
    "            is_valid: bool = Field(description=\"if the condition is satisfied\")\n",
    "\n",
    "        parser = PydanticOutputParser(pydantic_object=Validation)\n",
    "        input = f\"Answer the user query.\\n{parser.get_format_instructions()}\\n{validation_query}\\n\"\n",
    "        return parser.parse(self.llm.run(input)).is_valid\n",
    "\n",
    "    def parse(self, input):\n",
    "        \"\"\"ask the llm to parse the parse_class, based on the parse_prompt, from the input\"\"\"\n",
    "        parse_query = f'{self.parse_prompt}:\\n\\n\"{input}\"'\n",
    "        parser = PydanticOutputParser(pydantic_object=self.parse_class)\n",
    "        input = f\"Answer the user query.\\n{parser.get_format_instructions()}\\n{parse_query}\\n\"\n",
    "\n",
    "        return parser.parse(self.llm.run(input))\n",
    "\n",
    "    def execute_(self, input):\n",
    "        \"\"\"Executes the entire edge\n",
    "        returns a dictionary:\n",
    "        {\n",
    "            continue: bool,       weather or not should continue to next\n",
    "            result: parse_class,  the parsed result, if applicable\n",
    "            num_fails: int         the number of failed attempts\n",
    "        }\n",
    "        \"\"\"\n",
    "       \n",
    "        # input did't make it past the input condition for the edge\n",
    "        if not self.check(input):\n",
    "            \n",
    "            self.num_fails += 1\n",
    "            if self.num_fails >= self.max_retrys:\n",
    "                return {\"continue\": True, \"result\": None, \"num_fails\": self.num_fails}\n",
    "            return {\"continue\": False, \"result\": None, \"num_fails\": self.num_fails}\n",
    "\n",
    "        try:\n",
    "            # attempting to parse\n",
    "     \n",
    "            self.num_fails = 0\n",
    "            return {\n",
    "                \"continue\": True,\n",
    "                \"result\": self.parse(input),\n",
    "                \"num_fails\": self.num_fails,\n",
    "            }\n",
    "        except:\n",
    "            # there was some error in parsing.\n",
    "            # note, using the retry or correction parser here might be a good idea\n",
    "            self.num_fails += 1\n",
    "            if self.num_fails >= self.max_retrys:\n",
    "                return {\"continue\": True, \"result\": None, \"num_fails\": self.num_fails}\n",
    "            return {\"continue\": False, \"result\": None, \"num_fails\": self.num_fails}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f0edce8-df67-4a67-a806-a42bc589c0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining the query for the condition, and parse prompt\n",
    "condition = \"Does the input contain fruits?\"\n",
    "parse_prompt = \"extract only the fruits from the following text. Do not extract any food items besides pure fruits.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76e75004-3356-4826-b0c0-1c1766cbd67f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining the edge\n",
    "testEdge = Edge(\n",
    "    condition=condition,\n",
    "    parse_prompt=parse_prompt,\n",
    "    parse_class=sampleOutputTemplate,\n",
    "    llm=llm_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dbb0dcb-a72e-4710-bbd0-d753a8ec44c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a sample input from the user\n",
    "sample_input = \"my favorite deserts are chocolate covered strawberries, oreos, bannana splits, and cake.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6729e7bf-acf2-44de-8da6-41ab9f8b5237",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== testing the condition functionality =====\n",
      "weather or not the input \n",
      "\"my favorite deserts are chocolate covered strawberries, oreos, bannana splits, and cake.\"\n",
      "satisfies the condition\n",
      "\"Does the input contain fruits?\"\n",
      "result: True\n"
     ]
    }
   ],
   "source": [
    "print(\"===== testing the condition functionality =====\")\n",
    "print(\n",
    "    f'weather or not the input \\n\"{sample_input}\"\\nsatisfies the condition\\n\"{condition}\"'\n",
    ")\n",
    "print(\"result: {}\".format(testEdge.check(sample_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e617a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: {'continue': True, 'result': sampleOutputTemplate(output='strawberries, bannana'), 'num_fails': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"result: {}\".format(testEdge.execute_(sample_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27ce6bb8-35e0-4f9d-89b7-ffbacd0ec35e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== parse results =====\n",
      "strawberries, bannana\n"
     ]
    }
   ],
   "source": [
    "print(\"===== parse results =====\")\n",
    "print(testEdge.parse(sample_input).output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1673050f-5b67-4f0f-90b7-f9da00e3629b",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9263591-6f55-4214-b8d4-fc87bed92463",
   "metadata": {},
   "source": [
    "Now that we have an Edge, which handles input validation and parsing, we can define a Node, which handles conversational state. The Node requests a user for input, and passes that input to the directed edges coming from that Node. If none of the edges execute successfully, the Node asks the user for the input again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ce4fa11-e8f9-477c-9c33-01eb0c5b6443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    \"\"\"Node\n",
    "    at its highest level, a node asks a user for some input, and trys\n",
    "    that input on all edges. It also manages and executes all\n",
    "    the edges it contains\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prompt, retry_prompt):\n",
    "        \"\"\"\n",
    "        prompt (str): what to ask the user\n",
    "        retry_prompt (str): what to ask the user if all edges fail\n",
    "        parse_class (Pydantic BaseModel): the structure of the parse\n",
    "        llm (LangChain LLM): the large language model being used\n",
    "        \"\"\"\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.retry_prompt = retry_prompt\n",
    "        self.edges = []\n",
    "\n",
    "    def run_to_continue(self, _input):\n",
    "        \"\"\"Run all edges until one continues\n",
    "        returns the result of the continuing edge, or None\n",
    "        \"\"\"\n",
    "   \n",
    "        for edge in self.edges:\n",
    "            res = edge.execute_(_input)\n",
    "            print(res)\n",
    "            if res[\"continue\"]:\n",
    "                return res\n",
    "        return None\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"Handles the current conversational state\n",
    "        prompots the user, tries again, runs edges, etc.\n",
    "        returns the result from an adge\n",
    "        \"\"\"\n",
    "\n",
    "        # initial prompt for the conversational state\n",
    "        system_output(self.prompt)\n",
    "\n",
    "        while True:\n",
    "            # getting users input\n",
    "            _input = user_input()\n",
    "\n",
    "            # running through edges\n",
    "       \n",
    "            res = self.run_to_continue(_input)\n",
    "\n",
    "            if res is not None:\n",
    "                # parse successful\n",
    "                parsing_info(f\"parse results: {res}\")\n",
    "                return res\n",
    "\n",
    "            # unsuccessful, prompting retry\n",
    "            system_output(self.retry_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f99797-a64f-49ea-b681-8e6b637cfb47",
   "metadata": {},
   "source": [
    "With this implemented, we can begin seeing conversations take place. Weâ€™ll implement a Node which requests contact information, and two edges: one which attempts to parse out a valid email, and one that attempts to parse out a valid phone number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f8c11-1d0a-4fae-b59c-486fda7c27aa",
   "metadata": {},
   "source": [
    "## Connecting Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "012920d9-c60a-4d25-ab22-29a16eed3670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining 2 edges from the node\n",
    "\n",
    "condition1 = \"Does the input contain an email id?\"\n",
    "parse_prompt1 = \"extract the email from the following text.\"\n",
    "edge1 = Edge(condition1, parse_prompt1, sampleOutputTemplate, llm_chain)\n",
    "condition2 = (\n",
    "    \"Does the input contain a full and valid phone number (xxx-xxx-xxxx or xxxxxxxxxx)?\"\n",
    ")\n",
    "parse_prompt2 = \"extract the phone number from the following text.\"\n",
    "edge2 = Edge(condition2, parse_prompt2, sampleOutputTemplate, llm_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4ca7454-b6db-47d7-9174-b3e77c4432ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining A Node\n",
    "test_node = Node(\n",
    "    prompt=\"Please input your full email address or phone number\",\n",
    "    retry_prompt=\"I'm sorry, I didn't understand your response.\\nPlease provide a full email address or phone number(in the format xxx-xxx-xxxx)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21a5c026-10a5-4b68-8973-728a75ceddf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining Connections\n",
    "test_node.edges = [edge1, edge2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1714a934-adfd-4e3c-a0a3-649c396313d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Bot =======\n",
      "Please input your full email address or phone number\n",
      "======= Human Input =======\n",
      "{'continue': True, 'result': sampleOutputTemplate(output='michaeljackson@gmail.com'), 'num_fails': 0}\n",
      "*Info* parse results: {'continue': True, 'result': sampleOutputTemplate(output='michaeljackson@gmail.com'), 'num_fails': 0}\n"
     ]
    }
   ],
   "source": [
    "# running node. This handles all i/o and the logic to re-ask on failure.\n",
    "res = test_node.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecddc960-8c07-4889-9524-f2575a79f546",
   "metadata": {},
   "source": [
    "# Customer Support flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fee75c9e-9277-4b89-a10c-ba0b05d17743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data.chat import *\n",
    "from data.graph import *\n",
    "from data.validation import PhoneCallTicket, UserProfile, PhoneCallRequest\n",
    "from graph.chain_based_edge import *\n",
    "from graph.chain_based_node import *\n",
    "from graph.edge import BaseEdge\n",
    "from graph.node import BaseNode\n",
    "from graph.text_based_edge import PydanticTextBasedEdge\n",
    "from tools.rag_responder import HelpCenterAgent\n",
    "from tools.user_info_db import search_user_info_on_db, search_user_subscription_on_db\n",
    "from tools.audio_transcribe import call_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c2b6a521-618c-45e0-bf40-2bb959838d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "27aa0134-1b04-47fa-8668-409e6263bbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_message_history(msg_input: str, role: Role):\n",
    "    message_history = MessageHistory([])\n",
    "    message_history.add_message(msg_input, role)\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3b645d2d-55c8-42e5-8e12-084eb4706446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GreetingNode(BaseNode[str]):\n",
    "    STATIC_PROMPT = [\n",
    "        \"Hi, welcome to our online support, in order to proceed we need to identify you first, \"\n",
    "        \"could you please input your full email address or phone number\"\n",
    "    ]\n",
    "    RETRY_PROMPT = [\n",
    "        \"I'm sorry, I didn't understand your response.\"\n",
    "        \"\\nPlease provide a full email address or phone number(in the format xxx-xxx-xxxx)\"\n",
    "    ]\n",
    "\n",
    "    def greeting_message(self) -> Optional[MessageOutput]:\n",
    "        prompt = random.choice(self.STATIC_PROMPT)\n",
    "        return MessageOutput(prompt, role=Role.ASSISTANT)\n",
    "\n",
    "    def no_edges_found(self, **kwargs) -> Optional[MessageOutput]:\n",
    "        prompt = random.choice(self.RETRY_PROMPT)\n",
    "        return MessageOutput(prompt, role=Role.ASSISTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "181136a2-fefd-4d21-9e08-059aacef399b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class BaseNode(abc.ABC, Generic[NodeInput]):\n",
      "\n",
      "    \"\"\"Node\n",
      "    at it's highest level, a node asks a user for some input, and trys\n",
      "    that input on all edges. It also manages and executes all\n",
      "    the edges it contains\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, edges: Optional[List[BaseEdge]] = None, final_state=False):\n",
      "        \"\"\"\n",
      "        prompt (str): what to ask the user\n",
      "        retry_prompt (str): what to ask the user if all edges fail\n",
      "        parse_class (Pydantic BaseModel): the structure of the parse\n",
      "        llm (LangChain LLM): the large language model being used\n",
      "        \"\"\"\n",
      "\n",
      "        self._edges = edges\n",
      "        self._node_input = None\n",
      "        self._final_state = final_state\n",
      "\n",
      "    def is_node_final(self):\n",
      "        return self._final_state\n",
      "\n",
      "    def set_node_input(self, edge_output: EdgeOutput):\n",
      "        self._node_input = edge_output\n",
      "\n",
      "    def run_to_continue(self, user_input: NodeInput) -> Optional[EdgeOutput]:\n",
      "        \"\"\"Run all edges until one continues\n",
      "        returns the result of the continuing edge, or None\n",
      "        \"\"\"\n",
      "        res = None\n",
      "        for edge in self._edges:\n",
      "            res = edge.execute(user_input)\n",
      "            if res is not None and res.should_continue:\n",
      "                return res\n",
      "        return res\n",
      "\n",
      "    def execute(self, user_input: NodeInput) -> Union[MessageOutput, EdgeOutput]:\n",
      "        \"\"\"Handles the current conversational state\n",
      "        prompts the user, tries again, runs edges, etc.\n",
      "        returns the result from an adge\n",
      "        \"\"\"\n",
      "        res = self.run_to_continue(user_input)\n",
      "        if res is None or not res.should_continue:\n",
      "            return self.no_edges_found(user_input)\n",
      "        else:\n",
      "            if res.next_node is not None:\n",
      "                res.next_node.set_node_input(res.result)\n",
      "\n",
      "        return res\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def greeting_message(self) -> Optional[MessageOutput]:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def no_edges_found(self, user_input: NodeInput) -> Optional[MessageOutput]:\n",
      "        pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(BaseNode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2181210f-4d35-44d1-8318-698434c30f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "greeting_node = GreetingNode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f27c73bf-b623-4951-9083-1d0811890037",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MessageOutput(message='Hi, welcome to our online support, in order to proceed we need to identify you first, could you please input your full email address or phone number', role=<Role.ASSISTANT: 'assistant'>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greeting_node.greeting_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4968c8b9-9c59-48e5-84e6-93cdcd7d85d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UserInfoChainBasedEdge(ZeroShotChainBasedEdge):\n",
    "    _prompt_prefix = \"\"\"Your goal is to find out the user information and their subscription type.\n",
    "- You MUST ALWAYS combine the output of different tools to achieve your final answer.\n",
    "- The user subscription must be either free or premium, never empty\n",
    "\n",
    "To achieve this you have access to the following tools:\"\"\"\n",
    "\n",
    "    _prompt_suffix = \"\"\"\\nYour final answer should combine the information of previous Observations \n",
    "{format_instructions}\n",
    "Begin! \n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "    def _get_tools(self):\n",
    "        tools = [\n",
    "            Tool.from_function(\n",
    "                func=search_user_info_on_db,\n",
    "                description=\"Database tool to search user information, like user id, phone number and etc.Input should be their email as text\",\n",
    "                name=\"user_info_db_search\",\n",
    "            ),\n",
    "            Tool.from_function(\n",
    "                func=search_user_subscription_on_db,\n",
    "                description=\"Database tool to search user subscription type by user id, requires a number as input,\",\n",
    "                name=\"user_subscription_db_search\",\n",
    "            ),\n",
    "        ]\n",
    "        return tools\n",
    "\n",
    "    def _get_message_output(\n",
    "        self, msg_input: Union[str, BaseModel]\n",
    "    ) -> List[MessageOutput]:\n",
    "        user_info = msg_input if isinstance(msg_input, str) else str(msg_input)\n",
    "        message = f\"User Info retrieved: {user_info}\"\n",
    "        return [MessageOutput]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4a061f1f-00a7-4097-9298-a73119793a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class BaseEdge(abc.ABC, Generic[EdgeInput, ResultsType]):\n",
      "    def __init__(self, model, max_retries=3, out_node=None):\n",
      "        self._llm_model = model\n",
      "\n",
      "        # how many times the edge has failed, for any reason, for deciding to skip\n",
      "        # when successful this resets to 0 for posterity.\n",
      "        self._num_fails = 0\n",
      "\n",
      "        # how many retrys are acceptable\n",
      "        self._max_retries = max_retries\n",
      "\n",
      "        # the node the edge directs towards\n",
      "        self._out_node = out_node\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_message_output(\n",
      "        self, msg_input: Union[str, BaseModel]\n",
      "    ) -> Optional[List[MessageOutput]]:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def check(self, model_output: str) -> bool:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _parse(self, model_input: EdgeInput) -> ResultsType:\n",
      "        pass\n",
      "\n",
      "    def _get_edge_output(\n",
      "        self, should_continue: bool, result: Optional[ResultsType]\n",
      "    ) -> EdgeOutput:\n",
      "        message_output = self._get_message_output(result)\n",
      "        return EdgeOutput(\n",
      "            should_continue=should_continue,\n",
      "            result=result,\n",
      "            num_fails=self._num_fails,\n",
      "            next_node=self._out_node,\n",
      "            message_output=message_output,\n",
      "        )\n",
      "\n",
      "    def execute(self, user_input: EdgeInput):\n",
      "        \"\"\"Executes the entire edge\n",
      "        returns a dictionary:\n",
      "        {\n",
      "            continue: bool,       weather or not should continue to next\n",
      "            result: parse_class,  the parsed result, if applicable\n",
      "            num_fails: int        the number of failed attempts\n",
      "            continue_to: Node     the Node the edge continues to\n",
      "        }\n",
      "        \"\"\"\n",
      "\n",
      "        try:\n",
      "            # attempting to parse\n",
      "            self._num_fails = 0\n",
      "            return self._get_edge_output(\n",
      "                should_continue=True, result=self._parse(user_input)\n",
      "            )\n",
      "        except OutputParserException as parsing_exception:\n",
      "            # there was some error in parsing.\n",
      "            # note, using the retry or correction parser here might be a good idea\n",
      "            self._num_fails += 1\n",
      "            if self._num_fails >= self._max_retries:\n",
      "                return self._get_edge_output(\n",
      "                    should_continue=True,\n",
      "                    result=MessageOutput(\n",
      "                        parsing_exception.llm_output, role=Role.SYSTEM\n",
      "                    ),\n",
      "                )\n",
      "            return self._get_edge_output(\n",
      "                should_continue=False,\n",
      "                result=MessageOutput(parsing_exception.llm_output, role=Role.SYSTEM),\n",
      "            )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(BaseEdge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1c61327-af29-4275-b445-167d6822ac8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ChainBasedEdge(BaseEdge[MessageHistory, MessageOutput], ABC):\n",
      "    def __init__(\n",
      "        self,\n",
      "        model,\n",
      "        pydantic_object: Optional[Type[BaseModel]],\n",
      "        max_retries=3,\n",
      "        out_node=None,\n",
      "    ):\n",
      "        super().__init__(model=model, max_retries=max_retries, out_node=out_node)\n",
      "        if pydantic_object is not None:\n",
      "            self._output_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
      "        else:\n",
      "            self._output_parser = None\n",
      "\n",
      "        self._init_chain()\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _predict(self, model_input: ModelInput) -> str:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _init_chain(self, *kwargs):\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_prompt_template(self) -> BasePromptTemplate:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _prompt_input_variables(self) -> list:\n",
      "        pass\n",
      "\n",
      "    def check(self, model_output: str) -> bool:\n",
      "        return isinstance(self._output_parser.parse(model_output), BaseModel)\n",
      "\n",
      "    def _parse(self, message_history: MessageHistory) -> Union[str, BaseModel]:\n",
      "        model_input = message_history.model_input()\n",
      "        str_to_parse = self._predict(model_input=model_input)\n",
      "        out = (\n",
      "            self._output_parser.parse(str_to_parse)\n",
      "            if self._output_parser is not None\n",
      "            else str_to_parse\n",
      "        )\n",
      "        return out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(ChainBasedEdge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f86b832-c419-443b-ae0e-710a624ae2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ZeroShotChainBasedEdge(ChainBasedEdge, ABC):\n",
      "    _prompt_prefix = None\n",
      "    _prompt_suffix = None\n",
      "\n",
      "    def _prompt_input_variables(self):\n",
      "        input_variables = [\"input\", \"agent_scratchpad\", \"history\"]\n",
      "        if self._output_parser is not None:\n",
      "            input_variables += [\"format_instructions\"]\n",
      "\n",
      "        return input_variables\n",
      "\n",
      "    def _get_prompt_template(self) -> BasePromptTemplate:\n",
      "        input_variables = self._prompt_input_variables()\n",
      "        history = \"\"\"Conversation History \\n{history}\\n\"\"\"\n",
      "\n",
      "        prompt = ZeroShotAgent.create_prompt(\n",
      "            tools=self._tools,\n",
      "            prefix=self._prompt_prefix,\n",
      "            suffix=history + self._prompt_suffix,\n",
      "            input_variables=input_variables,\n",
      "        )\n",
      "        return prompt\n",
      "\n",
      "    def _init_chain(self, **kwargs):\n",
      "        self._tools = self._get_tools()\n",
      "\n",
      "        self._prompt = self._get_prompt_template()\n",
      "        self._llm_chain = LLMChain(llm=self._llm_model, prompt=self._prompt)\n",
      "\n",
      "        agent = ZeroShotAgent(llm_chain=self._llm_chain, tools=self._tools)\n",
      "        self._agent_executor = AgentExecutor.from_agent_and_tools(\n",
      "            agent=agent, tools=self._tools, verbose=True, handle_parsing_errors=True\n",
      "        )\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_tools(self):\n",
      "        pass\n",
      "\n",
      "    def _predict(self, model_input: ModelInput) -> str:\n",
      "        if self._output_parser is not None:\n",
      "            result = self._agent_executor.run(\n",
      "                input=model_input.input,\n",
      "                history=model_input.history,\n",
      "                format_instructions=self._output_parser.get_format_instructions(),\n",
      "            )\n",
      "        else:\n",
      "            result = self._agent_executor.run(\n",
      "                input=model_input.input, history=model_input.history\n",
      "            )\n",
      "\n",
      "        return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(ZeroShotChainBasedEdge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1edf9ec2-7e6f-498b-ac6d-730403cca81c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_info_edge = UserInfoChainBasedEdge(model=llm_model, pydantic_object=UserProfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f7eb78e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: We need to first search for the user information using their email and then find their subscription type.\n",
      "Action: user_info_db_search\n",
      "Action Input: michaeljackson@gmail.com\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[{'name': 'Michael Jackson', 'email': 'michaeljackson@gmail.com', 'user_id': '1', 'phone': '0452 333 666', 'language': 'English'}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWe have found the user information, now we need to search for their subscription type using the user id.\n",
      "Action: user_subscription_db_search\n",
      "Action Input: 1\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m[{'user_id': '1', 'subscription': 'premium'}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: \n",
      "{\n",
      "  \"name\": \"Michael Jackson\",\n",
      "  \"email\": \"michaeljackson@gmail.com\",\n",
      "  \"subscription\": \"premium\",\n",
      "  \"user_id\": 1,\n",
      "  \"phone\": \"0452 333 666\",\n",
      "  \"language\": \"English\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EdgeOutput(should_continue=True, result=UserProfile(name='Michael Jackson', email='michaeljackson@gmail.com', subscription='premium', user_id=1, phone='0452 333 666', language='English'), message_output=[<class 'data.graph.MessageOutput'>], num_fails=0, next_node=None)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info_edge.execute(get_message_history(\"michaeljackson@gmail.com\", Role.USER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5c10555-ed8c-4825-ad09-94976d362df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AuthenticatedUserNode(MultiRetrievalNode):\n",
    "    STATIC_PROMPT = [\n",
    "        \"Hi, {user_name} I am your Shopify Agent for today, you have the {subscription} subscription \"\n",
    "        \"I can help you with any Help or you can ask me to call you at anytime!\"\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model,\n",
    "        pydantic_object: Optional[Type[BaseNode]],\n",
    "        edges: List[BaseEdge] = None,\n",
    "    ):\n",
    "        self._hc_agent = HelpCenterAgent()\n",
    "        super().__init__(llm_model, pydantic_object, edges)\n",
    "\n",
    "    def greeting_message(self) -> Optional[MessageOutput]:\n",
    "        prompt = random.choice(self.STATIC_PROMPT)\n",
    "        user_profile: UserProfile = self._node_input\n",
    "\n",
    "        prompt = prompt.format(\n",
    "            user_name=user_profile.name, subscription=user_profile.subscription\n",
    "        )\n",
    "        return MessageOutput(prompt, role=Role.ASSISTANT)\n",
    "\n",
    "    def _get_retriever_infos(self):\n",
    "        retriever_infos = [\n",
    "            {\n",
    "                \"name\": \"Premium Subscription Knowledge Base\",\n",
    "                \"description\": \"Contains information for user with a premium subscription\",\n",
    "                \"retriever\": self._hc_agent.paid_sub_retriever(),\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Free Subscription Knowledge Base\",\n",
    "                \"description\": \"Contains information for user with a free subscription\",\n",
    "                \"retriever\": self._hc_agent.free_sub_retriever(),\n",
    "            },\n",
    "        ]\n",
    "        return retriever_infos\n",
    "\n",
    "    def _get_default_chain(self):\n",
    "        template = \"\"\"You are a helpful assistant, you should tell the user that his query is outside of your domain \n",
    "    in a friendly way\"\n",
    "    Human: \"\"\"\n",
    "\n",
    "        prompt_template = PromptTemplate.from_template(template)\n",
    "        chain = LLMChain(\n",
    "            llm=self._llm_model, prompt=prompt_template, output_key=\"result\"\n",
    "        )\n",
    "        return chain\n",
    "\n",
    "    def no_edges_found(self, user_input: MessageHistory) -> Optional[MessageOutput]:\n",
    "        message = self._predict(user_input)\n",
    "        return MessageOutput(message=message, role=Role.ASSISTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "acc5d6bb-1317-4c03-a013-ab4d60675d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ChainBasedNode(BaseNode[MessageHistory], abc.ABC):\n",
      "    def __init__(\n",
      "        self,\n",
      "        llm_model,\n",
      "        pydantic_object: Optional[Type[BaseModel]],\n",
      "        edges: Optional[List[BaseEdge]],\n",
      "        final_state=False,\n",
      "    ):\n",
      "        self._llm_model = llm_model\n",
      "        self._parse_class = pydantic_object\n",
      "\n",
      "        if pydantic_object is not None:\n",
      "            self._output_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
      "        else:\n",
      "            self._output_parser = None\n",
      "\n",
      "        self._init_chain()\n",
      "        super().__init__(edges, final_state)\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _init_chain(self, **kwargs):\n",
      "        pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(ChainBasedNode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57b433e2-0927-41f3-8a69-430b71a1fa77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class MultiRetrievalNode(ChainBasedNode, abc.ABC):\n",
      "    @abc.abstractmethod\n",
      "    def _get_retriever_infos(self):\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_default_chain(self):\n",
      "        pass\n",
      "\n",
      "    def _init_chain(self, *kwargs):\n",
      "        retriever_infos = self._get_retriever_infos()\n",
      "\n",
      "        self._llm_chain = MultiRetrievalQAChain.from_retrievers(\n",
      "            self._llm_model,\n",
      "            retriever_infos,\n",
      "            default_chain=self._get_default_chain(),\n",
      "            verbose=True,\n",
      "        )\n",
      "\n",
      "    def _predict(self, messages: MessageHistory) -> str:\n",
      "        return self._llm_chain.run(messages)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(MultiRetrievalNode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2aa7ca62-bb01-465d-91a6-018f044aa617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "authenticated_user_node = AuthenticatedUserNode(\n",
    "    llm_model=llm_model, pydantic_object=None, edges=[]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21347aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRetrievalQAChain chain...\u001b[0m\n",
      "Premium Subscription Knowledge Base: {'query': 'What is a POS'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MessageOutput(message='A POS stands for Point of Sale. It refers to the location where a transaction takes place, typically where a customer makes a payment for goods or services. In the context provided, Shopify POS is a point of sale app that allows businesses to sell products in person in various locations.', role=<Role.ASSISTANT: 'assistant'>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authenticated_user_node.execute(\n",
    "    get_message_history(\"User with Premium Subscription: What is a POS\", Role.USER)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3769f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
